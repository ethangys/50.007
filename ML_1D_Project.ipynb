{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read content from files\n",
    "with open(\"ES/train\", 'rb') as file:\n",
    "    EStrain_content = file.read()\n",
    "\n",
    "with open(\"RU/train\", 'rb') as file:\n",
    "    RUtrain_content = file.read()\n",
    "\n",
    "with open(\"ES/dev.in\", 'rb') as file:\n",
    "    ESin_content = file.read()\n",
    "\n",
    "with open(\"RU/dev.in\", 'rb') as file:\n",
    "    RUin_content = file.read()\n",
    "\n",
    "# Convert binary content to UTF-8 encoded lines\n",
    "estrain = [line.decode('utf-8') for line in EStrain_content.split(b'\\n')]\n",
    "rutrain = [line.decode('utf-8') for line in RUtrain_content.split(b'\\n')]\n",
    "esin = [line.decode('utf-8') for line in ESin_content.split(b'\\n')]\n",
    "ruin = [line.decode('utf-8') for line in RUin_content.split(b'\\n')]\n",
    "\n",
    "def estimate_emissions(data, k=1):\n",
    "    sentiment_count = {}\n",
    "    emission_parameters = {}\n",
    "    \n",
    "    for line in data:\n",
    "        if line == \"\":\n",
    "            continue\n",
    "        else:\n",
    "            word, sentiment = line.rsplit(' ', 1)\n",
    "            # Count sentiment occurrences\n",
    "            sentiment_count.setdefault(sentiment, 0)\n",
    "            sentiment_count[sentiment] += 1\n",
    "            \n",
    "            # Build emission_parameters dictionary\n",
    "            emission_parameters.setdefault(word, {}).setdefault(sentiment, 0)\n",
    "            emission_parameters[word][sentiment] += 1\n",
    "    \n",
    "    # Handle unknown word emissions\n",
    "    emission_parameters[\"#UNK#\"] = {}\n",
    "    for sentiment in sentiment_count:\n",
    "        emission_parameters['#UNK#'][sentiment] = k\n",
    "        sentiment_count[sentiment] += k\n",
    "    \n",
    "    # Divide by count\n",
    "    for word, sentiment_probs in emission_parameters.items():\n",
    "        for sentiment, count in sentiment_probs.items():\n",
    "            sentiment_probs[sentiment] = count / sentiment_count[sentiment]\n",
    "    \n",
    "    return emission_parameters\n",
    "\n",
    "def sentiment_analysis(data):\n",
    "    params = estimate_emissions(data)\n",
    "    tag_dict = {}\n",
    "    for word, sentiment_probs in params.items():\n",
    "        # Assign the most probable sentiment tag to each word\n",
    "        most_probable_tag = max(sentiment_probs, key=sentiment_probs.get)\n",
    "        tag_dict[word] = most_probable_tag\n",
    "    return tag_dict\n",
    "\n",
    "# Perform sentiment analysis\n",
    "estags = sentiment_analysis(estrain)\n",
    "rutags = sentiment_analysis(rutrain)\n",
    "\n",
    "def write_to_file(file_path, data_list):\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        for item in data_list:\n",
    "            file.write(f\"{item}\\n\")\n",
    "            \n",
    "# Write output files for ES\n",
    "output_es = []\n",
    "for word in esin:\n",
    "    if word == \"\":\n",
    "        line = \"\"\n",
    "    elif word not in estags:\n",
    "        sentiment = estags[\"#UNK#\"]\n",
    "        line = f\"{word} {sentiment}\"\n",
    "    else:\n",
    "        sentiment = estags[word]\n",
    "        line = f\"{word} {sentiment}\"\n",
    "    output_es.append(line)\n",
    "\n",
    "write_to_file(\"ES/dev.p1.out\", output_es)\n",
    "\n",
    "# Write output files for RU\n",
    "output_ru = []\n",
    "for word in ruin:\n",
    "    if word == \"\":\n",
    "        sentiment = \"\"\n",
    "    elif word not in rutags:\n",
    "        sentiment = rutags[\"#UNK#\"]\n",
    "    else:\n",
    "        sentiment = rutags[word]\n",
    "    line = f\"{word} {sentiment}\"\n",
    "    output_ru.append(line)\n",
    "\n",
    "write_to_file(\"RU/dev.p1.out\", output_ru)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def estimate_transition(data):\n",
    "    count = {}  # Count occurrences of state transitions\n",
    "    params = {}  # Store transition probabilities\n",
    "    states = []  # Store states for each sentence\n",
    "    sentence = []  # Temporary storage for a sentence\n",
    "    \n",
    "    # Parse input data\n",
    "    for line in data:\n",
    "        if line != \"\":\n",
    "            sentence.append(line.split()[-1])\n",
    "        elif sentence != [] and line == \"\":\n",
    "            states.append(sentence)\n",
    "            sentence = []\n",
    "    \n",
    "    # Calculate counts and probabilities\n",
    "    for sentence in states:\n",
    "        if \"START\" not in count:\n",
    "            count[\"START\"] = 1\n",
    "        else:\n",
    "            count[\"START\"] += 1\n",
    "    \n",
    "        if (\"START\", sentence[0]) not in params:\n",
    "            params[(\"START\", sentence[0])] = 1\n",
    "        else:\n",
    "            params[(\"START\", sentence[0])] += 1\n",
    "        \n",
    "        for i in range(len(sentence)):\n",
    "            if sentence[i] not in count:\n",
    "                count[sentence[i]] = 1\n",
    "            else:\n",
    "                count[sentence[i]] += 1\n",
    "            if i != 0:\n",
    "                if (sentence[i-1], sentence[i]) not in params:\n",
    "                    params[(sentence[i-1], sentence[i])] = 1\n",
    "                else:\n",
    "                    params[(sentence[i-1], sentence[i])] += 1\n",
    "        if \"STOP\" not in count:\n",
    "            count[\"STOP\"] = 1\n",
    "        else:\n",
    "            count[\"STOP\"] += 1\n",
    "        if (sentence[-1], \"STOP\") not in params:\n",
    "            params[(sentence[-1], \"STOP\")] = 1\n",
    "        else:\n",
    "            params[(sentence[-1], \"STOP\")] += 1\n",
    "    \n",
    "    # Divide by count\n",
    "    for pair in params:\n",
    "        params[pair] = params[pair] / count[pair[0]]\n",
    "    \n",
    "    return params\n",
    "\n",
    "# Estimate emissions and transitions for ES and RU training data\n",
    "es_e_params = estimate_emissions(estrain)\n",
    "es_t_params = estimate_transition(estrain)\n",
    "ru_e_params = estimate_emissions(rutrain)\n",
    "ru_t_params = estimate_transition(rutrain)\n",
    "\n",
    "def viterbi(e_params, t_params, sentence):\n",
    "    n = len(sentence)\n",
    "    path = []  # Store the resulting path\n",
    "    states = []  # List to hold possible states\n",
    "    policy = {}  # Dictionary to store optimal state transitions\n",
    "    \n",
    "    # Extract states from transition parameters\n",
    "    for pair in t_params:\n",
    "        states.append(pair[0])\n",
    "    states = set(states)  # Convert to set for faster lookup\n",
    "    \n",
    "    matrix = [{\"START\": 0}]  # Initialize the Viterbi matrix with \"START\" state\n",
    "    \n",
    "    # Perform Viterbi algorithm\n",
    "    for i in range(1, n + 1):\n",
    "        policy[i] = {}  # Initialize policy for current position\n",
    "        matrix.append({})\n",
    "        word = sentence[i - 1]\n",
    "        \n",
    "        # Handle unknown words by using \"#UNK#\" token\n",
    "        if word not in e_params:\n",
    "            word = \"#UNK#\"\n",
    "        \n",
    "        for v in e_params[word]:\n",
    "            matrix[i][v] = -np.inf  # Initialize with negative infinity\n",
    "            \n",
    "            for u in matrix[i - 1]:\n",
    "                if (u, v) not in t_params:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate the score using log probabilities\n",
    "                score = matrix[i - 1][u] + np.log(e_params[word][v]) + np.log(t_params[(u, v)])\n",
    "                \n",
    "                if score > matrix[i][v]:\n",
    "                    matrix[i][v] = score\n",
    "                    policy[i][v] = u  # Store the optimal previous state\n",
    "    \n",
    "    # Check for the presence of valid paths\n",
    "    for i in range(n):\n",
    "        if all(score == -np.inf for score in matrix[i].values()):\n",
    "            return [\"O\"] * n  # Return a list of \"O\" labels if no valid paths\n",
    "    \n",
    "    # Find the end state with the maximum score\n",
    "    end_state = max(matrix[n], key=lambda state: matrix[n][state])\n",
    "    prev_state = end_state\n",
    "    path.append(prev_state)  # Append the end state to the path\n",
    "    \n",
    "    # Trace back through the policy to find the best path\n",
    "    for i in range(n, 1, -1):\n",
    "        path.append(policy[i][prev_state])\n",
    "        prev_state = policy[i][prev_state]\n",
    "    \n",
    "    return path[::-1]  # Return the reversed path as the actual sequence of states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_write(input_data, e_params, t_params, output_file):\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    \n",
    "    # Split input lines into sentences\n",
    "    for line in input_data:\n",
    "        if line != \"\":\n",
    "            sentence.append(line)\n",
    "        elif line == \"\" and sentence != []:\n",
    "            sentences.append(sentence)\n",
    "            sentence = []\n",
    "    \n",
    "    # Predict sentiment using Viterbi algorithm for each sentence\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        for sentence in sentences:\n",
    "            path = viterbi(e_params, t_params, sentence)\n",
    "            \n",
    "            # Write predicted sentiment for each word\n",
    "            for word, sentiment in zip(sentence, path):\n",
    "                line = f\"{word} {sentiment}\\n\"\n",
    "                file.write(line)\n",
    "            file.write(\"\\n\")\n",
    "\n",
    "# Process ES dev.in data and write predictions to ES/dev.p2.out\n",
    "viterbi_write(esin, es_e_params, es_t_params, \"ES/dev.p2.out\")\n",
    "\n",
    "# Process RU dev.in data and write predictions to RU/dev.p2.out\n",
    "viterbi_write(ruin, ru_e_params, ru_t_params, \"RU/dev.p2.out\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Read training data from a file and collect emission and transition counts\n",
    "def read_file(filepath):\n",
    "    states = {}\n",
    "    e_count = {}\n",
    "    t_count = {}\n",
    "    train_x = set()\n",
    "\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        yi = \"START\"\n",
    "        states[\"START\"] = 1\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                t_count[(yi, \"STOP\")] = t_count.get((yi, \"STOP\"), 0) + 1\n",
    "                states[\"STOP\"] = states.get(\"STOP\", 0) + 1\n",
    "                yi = \"START\"\n",
    "                states[\"START\"] = states.get(\"START\", 0) + 1\n",
    "                continue\n",
    "\n",
    "            idx = line.rfind(\" \")\n",
    "            x, yj = line[:idx], line[idx + 1:]\n",
    "            t_count[(yi, yj)] = t_count.get((yi, yj), 0) + 1\n",
    "            e_count[(yj, x)] = e_count.get((yj, x), 0) + 1\n",
    "            states[yj] = states.get(yj, 0) + 1\n",
    "            train_x.add(x)\n",
    "            yi = yj\n",
    "\n",
    "        if yi != \"START\":\n",
    "            t_count[(yi, \"STOP\")] = t_count.get((yi, \"STOP\"), 0) + 1\n",
    "            states[\"STOP\"] = states.get(\"STOP\", 0) + 1\n",
    "        else:\n",
    "            states[\"START\"] -= 1\n",
    "    return states, e_count, t_count, train_x\n",
    "\n",
    "# Calculate emission probabilities based on the given parameters\n",
    "def e_params(xt, yt, states, e_count, train_x, k=1):\n",
    "    if xt in train_x:\n",
    "        if e_count.get((yt, xt), 0) == 0:\n",
    "            return float(\"-inf\")\n",
    "        return math.log(e_count.get((yt, xt), 0) / (states[yt] + k))\n",
    "    else:\n",
    "        return math.log(k / (states[yt] + k))\n",
    "\n",
    "# Calculate transition probabilities based on the given parameters\n",
    "def t_params(yi, yj, t_count, states):\n",
    "    num = t_count.get((yi, yj), 0)\n",
    "    denom = states[yi]\n",
    "    if num == 0:\n",
    "        return float('-inf')\n",
    "    return math.log(num / denom)\n",
    "\n",
    "# Read input sequences from a file\n",
    "def read_in(file_path):\n",
    "    x_seq = []\n",
    "    curr_seq = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            x = line.strip()\n",
    "            if not x:\n",
    "                if curr_seq:\n",
    "                    x_seq.append(curr_seq)\n",
    "                    curr_seq = []\n",
    "            else:\n",
    "                curr_seq.append(x)\n",
    "        if curr_seq:\n",
    "            x_seq.append(curr_seq)\n",
    "    return x_seq\n",
    "\n",
    "# Write sequence pairs to a file\n",
    "def write_p_tofile(file_path, seq_list):\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        for seq_pairs in seq_list:\n",
    "            for x, y in seq_pairs:\n",
    "                f.write(f\"{x} {y}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "# Calculate k-best sequences using Viterbi algorithm\n",
    "def k_best_seq(states, e_count, t_count, train_x, x_input_seq, k=1):\n",
    "    n = len(x_input_seq)\n",
    "    s = list(states.keys())\n",
    "\n",
    "    scores = {}\n",
    "    for i in range(n + 1):\n",
    "        for state in s:\n",
    "            score_list = []\n",
    "            for j in range(k):\n",
    "                score_list.append((float('-inf'), None, None))\n",
    "            scores[(i, state)] = score_list\n",
    "\n",
    "    scores[(n + 1, \"STOP\")] = None\n",
    "    scores[(0, \"START\")] = [(0.0, None, None)]\n",
    "\n",
    "    for t in range(1, n + 1):\n",
    "        for v in s:\n",
    "            if v == \"START\" or v == \"STOP\":\n",
    "                continue\n",
    "            all_scores = []\n",
    "            for u in s:\n",
    "                if (u == \"STOP\") or (u == \"START\" and t != 1):\n",
    "                    continue\n",
    "                for idx, scorepair in enumerate(scores[(t - 1, u)]):\n",
    "                    emission_prob = e_params(x_input_seq[t - 1], v, states, e_count, train_x, 1)\n",
    "                    transition_prob = t_params(u, v, t_count, states)\n",
    "                    current_v_score = scorepair[0] + emission_prob + transition_prob\n",
    "                    all_scores.append((current_v_score, u, idx))\n",
    "            scores[(t, v)] = sorted(all_scores, reverse=True)[:k]\n",
    "\n",
    "        all_scores = []\n",
    "        for u in s:\n",
    "            if (u == \"START\") or (u == \"STOP\"):\n",
    "                continue\n",
    "            for idx, scorepair in enumerate(scores[(n, u)]):\n",
    "                transition_prob = t_params(u, \"STOP\", t_count, states)\n",
    "                current_v_score = scorepair[0] + transition_prob\n",
    "                all_scores.append((current_v_score, u, idx))\n",
    "        scores[(n + 1, \"STOP\")] = sorted(all_scores, reverse=True)[:k]\n",
    "\n",
    "    k_best_paths = []\n",
    "    score_list = [(n + 1, \"STOP\")]\n",
    "    for idx_in_STOP_list in range(k):\n",
    "        path = []\n",
    "        score, parent, idx_in_parent = scores[(n + 1, \"STOP\")][idx_in_STOP_list]\n",
    "        for i in range(n, 0, -1):\n",
    "            path.insert(0, parent)\n",
    "            score, parent, idx_in_parent = scores[(i, parent)][idx_in_parent]\n",
    "        k_best_paths.append(path)\n",
    "    return k_best_paths, scores\n",
    "\n",
    "# Perform computations on input data and write output paths to files\n",
    "def compute(dev_in_path, states, e_count, t_count, train_x, write_paths, k_paths):\n",
    "    x_seqs = read_in(dev_in_path)\n",
    "    k = max(k_paths)\n",
    "    seqs = {p: [] for p in k_paths}\n",
    "\n",
    "    for x_seq in x_seqs:\n",
    "        k_best_paths, scores = k_best_seq(states, e_count, t_count, train_x, x_seq, k)\n",
    "        for p in k_paths:\n",
    "            seqs[p].append(list(zip(x_seq, k_best_paths[p - 1])))\n",
    "\n",
    "    for write_loc, path_num in zip(write_paths, k_paths):\n",
    "        write_p_tofile(write_loc, seqs[path_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read training data and compute paths for RU language\n",
    "states_RU, e_count_RU, t_count_RU, train_x_RU = read_file(\"RU/train\")\n",
    "compute(\"RU/dev.in\", states_RU, e_count_RU, t_count_RU, train_x_RU, [\"RU/dev.p3.2nd.out\", \"RU/dev.p3.8th.out\"], [2, 8])\n",
    "\n",
    "# Read training data and compute paths for ES language\n",
    "states_ES, e_count_ES, t_count_ES, train_x_ES = read_file(\"ES/train\")\n",
    "compute(\"ES/dev.in\", states_ES, e_count_ES, t_count_ES, train_x_ES, [\"ES/dev.p3.2nd.out\", \"ES/dev.p3.8th.out\"], [2, 8])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_to_lists(data):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    current_sentence = []\n",
    "    current_labels = []\n",
    "    for line in data.split(\"\\n\"):\n",
    "        line = line.strip()  # Remove leading/trailing whitespace\n",
    "        if line:\n",
    "            token, label = line.rsplit(maxsplit=1)  # Split from the right\n",
    "            current_sentence.append(token)  # Convert word to lowercase\n",
    "            current_labels.append(label)\n",
    "        else:\n",
    "            if current_sentence:\n",
    "                sentences.append(current_sentence)\n",
    "                labels.append(current_labels)\n",
    "                current_sentence = []\n",
    "                current_labels = []\n",
    "\n",
    "    sentences_lower = [[word for word in sentence] for sentence in sentences]\n",
    "\n",
    "    return sentences_lower, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_log(e_params, t_params, sentence):\n",
    "    n = len(sentence)\n",
    "    path = []\n",
    "    states = []  # List to store unique states\n",
    "    policy = {}  # Dictionary to store the best path policy\n",
    "\n",
    "    # Extract unique states from transition parameters\n",
    "    for pair in t_params.keys():\n",
    "        states.append(pair[0])\n",
    "    states = set(states)  \n",
    "\n",
    "    matrix = [{\"START\": 0}]  # Initialize the Viterbi matrix with the \"START\" state\n",
    "    for i in range(1, n + 1):\n",
    "        policy[i] = {}  # Initialize policy for the current position\n",
    "        matrix.append({})\n",
    "        word = sentence[i - 1]\n",
    "        \n",
    "        # Handle unknown words by using \"#UNK#\" token\n",
    "        if word not in e_params.keys():\n",
    "            word = \"#UNK#\"\n",
    "        \n",
    "        # Iterate over possible states for the current word\n",
    "        for v in e_params[word]:\n",
    "            matrix[i][v] = -np.inf  # Initialize with negative infinity\n",
    "            for u in matrix[i - 1]:\n",
    "                if (u, v) not in t_params.keys():\n",
    "                    continue\n",
    "                score = matrix[i - 1][u] + e_params[word][v] + t_params[(u, v)]\n",
    "                if score > matrix[i][v]:\n",
    "                    matrix[i][v] = score\n",
    "                    policy[i][v] = u  # Update the best previous state\n",
    "                    \n",
    "    # Check for the presence of valid paths\n",
    "    for i in range(n):\n",
    "        if all(score == -np.inf for score in matrix[i].values()):\n",
    "            return [\"O\"] * n  # Return a list of \"O\" labels if no valid paths\n",
    "\n",
    "    # Find the end state with the maximum score\n",
    "    end_state = max(matrix[n], key=lambda state: matrix[n][state])\n",
    "    prev_state = end_state\n",
    "    path.append(prev_state)  # Append the end state to the path\n",
    "    \n",
    "    # Trace back through the policy to find the best path\n",
    "    for i in range(n, 1, -1):\n",
    "        path.append(policy[i][prev_state])\n",
    "        prev_state = policy[i][prev_state]\n",
    "    \n",
    "    return path[::-1]  # Return the reversed path as the actual sequence of states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ES/train\", 'rb') as file:\n",
    "    EStrain_content = file.read().decode(\"utf-8\")\n",
    "\n",
    "with open(\"RU/train\", 'rb') as file:\n",
    "    RUtrain_content = file.read().decode(\"utf-8\")\n",
    "\n",
    "logged_es_t_values = {}\n",
    "for key, value in es_t_params.items():\n",
    "    logged_es_t_values[key] = 0\n",
    "\n",
    "logged_es_e_values = {}\n",
    "for key, inner_dict in es_e_params.items():\n",
    "    logged_es_e_values[key] = {}\n",
    "    for inner_key, inner_value in inner_dict.items():\n",
    "        logged_es_e_values[key][inner_key] = 0\n",
    "\n",
    "logged_ru_t_values = {}\n",
    "for key, value in ru_t_params.items():\n",
    "    logged_ru_t_values[key] = 0     \n",
    "\n",
    "logged_ru_e_values = {}\n",
    "for key, inner_dict in ru_e_params.items():\n",
    "    logged_ru_e_values[key] = {}\n",
    "    for inner_key, inner_value in inner_dict.items():\n",
    "        logged_ru_e_values[key][inner_key] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "def train_struct_perceptron_with_smoothing(e_params, t_params, data, r, epochs, smoothing_factor=1):\n",
    "    sentences, labels = convert_data_to_lists(data)\n",
    "    \n",
    "    for k in range(epochs):\n",
    "        print(k)\n",
    "        for i in range(len(sentences)):\n",
    "            predicted_path = viterbi_log(e_params, t_params, sentences[i])\n",
    "            actual_path = labels[i]\n",
    "\n",
    "            updated_e_params = {word: {label: 0 for label in label_set} for word, label_set in e_params.items()}\n",
    "            updated_t_params = {label_pair: 0 for label_pair in t_params.keys()}\n",
    "            \n",
    "            for j in range(len(predicted_path)):\n",
    "                if predicted_path[j] != actual_path[j]:\n",
    "                    if j != 0 and j != (len(predicted_path) - 1):\n",
    "                        updated_t_params[(actual_path[j-1], actual_path[j])] += r\n",
    "                        updated_t_params[(actual_path[j], actual_path[j+1])] += r\n",
    "                        updated_e_params[sentences[i][j]][actual_path[j]] += r\n",
    "                        \n",
    "                        updated_t_params[(predicted_path[j-1], predicted_path[j])] -= r\n",
    "                        updated_t_params[(predicted_path[j], predicted_path[j+1])] -= r\n",
    "                        updated_e_params[sentences[i][j]][predicted_path[j]] -= r\n",
    "\n",
    "                    if j == 0:\n",
    "                        if (\"START\", actual_path[j]) not in t_params:\n",
    "                            updated_t_params[(\"START\", actual_path[j])] = 0\n",
    "                        updated_t_params[(\"START\", actual_path[j])] += r\n",
    "                        updated_t_params[(actual_path[j], actual_path[j+1])] += r\n",
    "                        updated_e_params[sentences[i][j]][actual_path[j]] += r\n",
    "        \n",
    "                        if (\"START\", predicted_path[j]) not in t_params:\n",
    "                            updated_t_params[(\"START\", predicted_path[j])] = 0\n",
    "                        updated_t_params[(\"START\", predicted_path[j])] -= r\n",
    "                        updated_t_params[(predicted_path[j], predicted_path[j+1])] -= r\n",
    "                        updated_e_params[sentences[i][j]][predicted_path[j]] -= r\n",
    "                    \n",
    "                    if j == (len(predicted_path)-1):\n",
    "                        updated_t_params[(actual_path[j-1], actual_path[j])] += r\n",
    "                        if (actual_path[j], \"STOP\") not in t_params:\n",
    "                            updated_t_params[(actual_path[j], \"STOP\")] = 0\n",
    "                        updated_t_params[(actual_path[j], \"STOP\")] += r\n",
    "                        updated_e_params[sentences[i][j]][actual_path[j]] += r\n",
    "                        updated_t_params[(predicted_path[j-1], predicted_path[j])] -= r\n",
    "                        if (predicted_path[j], \"STOP\") not in t_params:\n",
    "                            updated_t_params[(predicted_path[j], \"STOP\")] = 0\n",
    "                        updated_t_params[(predicted_path[j], \"STOP\")] -= r\n",
    "                        updated_e_params[sentences[i][j]][predicted_path[j]] -= r\n",
    "                            \n",
    "            \n",
    "            for word in updated_e_params:\n",
    "                for label in updated_e_params[word]:\n",
    "                    e_params[word][label] += updated_e_params[word][label]\n",
    "                    \n",
    "            for label_pair in updated_t_params:\n",
    "                if label_pair not in t_params:\n",
    "                    t_params[label_pair]=0\n",
    "                t_params[label_pair] += updated_t_params[label_pair]\n",
    "    \n",
    "    return e_params, t_params\n",
    "\n",
    "ru_new_weights_e,ru_new_weights_t=train_struct_perceptron_with_smoothing(logged_ru_e_values,logged_ru_t_values, RUtrain_content,0.1,15)\n",
    "es_new_weights_e, es_new_weights_t=train_struct_perceptron_with_smoothing(logged_es_e_values,logged_es_t_values, EStrain_content,0.1,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def struc_perceptron_write(input_lines, new_weights_e, new_weights_t, output_file):\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "\n",
    "    for line in input_lines:\n",
    "        if line.strip():\n",
    "            sentence.append(line.strip())\n",
    "        elif sentence:\n",
    "            sentences.append(sentence)\n",
    "            sentence = []\n",
    "\n",
    "    with open(output_file, \"w\", encoding='utf-8') as file:\n",
    "        for sentence in sentences:\n",
    "            path = viterbi_log(new_weights_e, new_weights_t, sentence)\n",
    "            for word, sentiment in zip(sentence, path):\n",
    "                line = f\"{word} {sentiment}\\n\"\n",
    "                file.write(line)\n",
    "            file.write(\"\\n\")\n",
    "\n",
    "struc_perceptron_write(esin, es_new_weights_e, es_new_weights_t, \"ES/dev.p4.out\")\n",
    "\n",
    "struc_perceptron_write(ruin, ru_new_weights_e, ru_new_weights_t, \"RU/dev.p4.out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ES/test.in\", \"rb\",) as file:\n",
    "    ES_test_content = file.read()\n",
    "estest = [line.decode('utf-8') for line in ES_test_content.split(b'\\n')]\n",
    "\n",
    "struc_perceptron_write(estest, es_new_weights_e, es_new_weights_t, \"ES/test.p4.out\")\n",
    "\n",
    "with open(\"RU/test.in\", \"rb\",) as file:\n",
    "    RU_test_content = file.read()\n",
    "rutest = [line.decode('utf-8') for line in RU_test_content.split(b'\\n')]\n",
    "\n",
    "struc_perceptron_write(rutest, ru_new_weights_e, ru_new_weights_t, \"RU/test.p4.out\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
